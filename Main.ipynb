{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEvTRtNnDY2Q",
        "outputId": "aa57d5b6-59d2-418b-ea4e-64b5df9c1a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS-3010-Final'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 140 (delta 61), reused 81 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (140/140), 18.29 MiB | 13.88 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/afrenkai/DS-3010-Final.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DS-3010-Final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiUGv3JMDiIb",
        "outputId": "f9729627-5ea2-45e7-be78-e943ed56132c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS-3010-Final/DS-3010-Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Exc780BDznq",
        "outputId": "db692f9a-0a78-4700-f551-88fd98470d53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cv.py  LICENSE\t   new_3010_proj_work_ben.ipynb  requirements.txt\n",
            "Data   Main.ipynb  README.md\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from lightgbm import LGBMRegressor\n",
        "from torcheval.metrics import R2Score"
      ],
      "metadata": {
        "id": "KjqLGK9uDuvW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('Data/SGEMM_train.csv')\n",
        "val_df = pd.read_csv('Data/SGEMM_val.csv')\n",
        "\n",
        "#TODO: read test data (already in data dir), see what's going on in lightgbm, get r2 for the neural net"
      ],
      "metadata": {
        "id": "jSFtZgO5EIYi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "yFsL0jLPEah0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x, xmin, xmax, a, b):\n",
        "  '''\n",
        "  Restricts x values to range of [xmin, xmax]\n",
        "  '''\n",
        "  numerator = x - xmin\n",
        "  denominator = xmax - xmin\n",
        "  return (numerator / denominator) * (b - a) + a"
      ],
      "metadata": {
        "id": "VeZMpJ4fWVnS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_combine = ['Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)']"
      ],
      "metadata": {
        "id": "Zg8_iVh1EoPf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df: pd.DataFrame):\n",
        "  df['DELTA_RUNTIME'] = df.apply(\n",
        "      lambda row: np.mean([row['Run1 (ms)'], row['Run2 (ms)'], row['Run3 (ms)'], row['Run4 (ms)']]),\n",
        "      axis=1\n",
        "  )\n",
        "  for col in df.columns:\n",
        "    if col in cols_to_combine:\n",
        "      df = df.drop(col, axis = 1) #removes redundant cols\n",
        "\n",
        "\n",
        "  min = 0\n",
        "  max = 1\n",
        "\n",
        "  df = df.apply(\n",
        "      lambda row: (norm(row, row.min(), row.max(), min, max))\n",
        "  )\n",
        "  x = df.iloc[:, :14] # features\n",
        "  y = df.iloc[:, -1:] # target\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "t7Ldc6DaEWb5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "DsGLn9WEE1Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_tr, y_tr = preprocess(train_df)\n",
        "\n",
        "train_data = lgb.Dataset(x_tr, label=y_tr)\n",
        "x_val, y_val = preprocess(val_df)\n",
        "# Create a LightGBM dataset for testing with features X_val and labels Y_val,\n",
        "# and specify the reference dataset as train_data for consistent evaluation\n",
        "val_data = lgb.Dataset(x_val, label=y_val, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "}\n",
        "\n",
        "num_round = 100\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[\n",
        "                val_data])\n",
        "\n",
        "\n",
        "# Create an instance of the LightGBM Regressor with the RMSE metric.\n",
        "model = LGBMRegressor(metric='mse')\n",
        "\n",
        "# Train the model using the training data.\n",
        "model.fit(x_tr, y_tr)\n",
        "\n",
        "y_train = model.predict(x_tr)\n",
        "y_v = model.predict(x_val)\n",
        "print(\"Training MSE:\", mse(y_tr, y_train))\n",
        "print(\"Validation MSE:\", mse(y_val, y_v))\n",
        "\n",
        "print('train r2:', r2_score(y_tr, y_train))\n",
        "print('val r2:', r2_score(y_val, y_v))"
      ],
      "metadata": {
        "id": "bWXyH5z-Lmnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c8c396-b4a9-4ab9-cd53-eef5bd4b5dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017275 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030284 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "Training MSE: 0.00012048252219381041\n",
            "Validation MSE: 0.0001191615359407208\n",
            "train r2: 0.9901529624802171\n",
            "val r2: 0.9903041142135522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Neural Net"
      ],
      "metadata": {
        "id": "GKjUYgsALmtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUNN(nn.Module):\n",
        "  def __init__(self, in_feat, out_feat):\n",
        "    super(GPUNN, self).__init__()\n",
        "    self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "    self.L1 = nn.Linear(in_feat, 64, device=self.device)\n",
        "    self.L2 = nn.Linear(64, out_feat, device = self.device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(64, device = self.device)\n",
        "    self.bn2 = nn.BatchNorm1d(out_feat, device = self.device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(self.relu(self.L1(x)))\n",
        "    x = self.bn2(self.L2(x))\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "TJtfhd5sLoz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training output example was basen on an example from pytorch on using CNNs for ImageNet. https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
        "# lines 45-48 in original code, lines roughly 13-16 in ours\n",
        "def train(model: nn.Module, train_dl: DataLoader, batch_size, device, n_epochs, optimizer, criterion):\n",
        "    model.train()\n",
        "    for epoch in trange(n_epochs):\n",
        "        for batch, (data, target) in enumerate(train_dl):\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch % 1000 == 0:\n",
        "                print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch + 1, batch * len(data), len(train_dl.dataset),\n",
        "                    100. * batch / len(train_dl), loss.item()))\n",
        "\n",
        "    torch.save(model.state_dict(), 'nn.pth')\n",
        "    print('model saved to nn.pth')\n"
      ],
      "metadata": {
        "id": "jrBA-CHTOLEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#r2 score stuff taken from https://pytorch.org/torcheval/main/generated/torcheval.metrics.R2Score.html\n",
        "#I think meaning loss over batches is fair, since that's a relatively good indicator of performance.\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    losses = []\n",
        "    metric = R2Score()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss = criterion(output, target)\n",
        "            # print('loss bien')\n",
        "            # print(f'Target Tensor: {target.detach().cpu().numpy()}\\n, Output Tensor:{output.detach().cpu().numpy()}\\n')\n",
        "\n",
        "            metric.update(output, target)\n",
        "            # print(f' r2 rn: {metric.compute()}')\n",
        "\n",
        "            # print('r2 bien')\n",
        "            losses.append(test_loss)\n",
        "\n",
        "\n",
        "    # print(test_loss / len(test_loader.dataset))\n",
        "    # print(np.mean(r2s))\n",
        "    r2 = metric.compute()\n",
        "    r2 = float(r2.detach().cpu().numpy())\n",
        "    # print(type(r2))\n",
        "    # print(round(r2, 4))\n",
        "    return (np.mean([ten.detach().cpu().numpy() for ten in losses]), round(r2, 4) )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-NN3jSkBVjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dls(x: pd.DataFrame, y:pd.DataFrame, split: str = 'train'):\n",
        "  x = x.loc[:, :].values #conv to np array\n",
        "  y = y.loc[:, :].values\n",
        "  x_ten = torch.tensor(x) # conv to tensor\n",
        "  y_ten = torch.tensor(y)\n",
        "  ds = TensorDataset(x_ten, y_ten)\n",
        "  if split == 'train':\n",
        "    dl = DataLoader(ds, batch_size = 32, shuffle = True) #safe to shuffle since its train\n",
        "  else:\n",
        "    dl = DataLoader(ds, batch_size = 32, shuffle = False) #assuming test/val, can't shuffle\n",
        "  return ds, dl"
      ],
      "metadata": {
        "id": "8ZQiw6qdP-5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr, y_tr = preprocess(train_df)\n",
        "model = GPUNN(len(x_tr.columns), len(y_tr.columns))\n",
        "print(model)\n",
        "_, train_dl = create_dls(x_tr, y_tr)\n",
        "criterion = nn.MSELoss() # MSE, like we've been using everywhere else\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4) # pretty standard, no need for k-fold since the method works pretty well without tuning\n",
        "train(model, train_dl, 32, model.device, 10, optimizer, criterion) # 10 epochs to get a baseline."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS8nqyTzNkF6",
        "outputId": "d59b5f9f-8fe9-416f-dc4b-fd58361796da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUNN(\n",
            "  (L1): Linear(in_features=14, out_features=64, bias=True)\n",
            "  (L2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 1 [0/193280 (0%)]\tLoss: 1.041921\n",
            "\n",
            "Train Epoch: 1 [32000/193280 (17%)]\tLoss: 0.748578\n",
            "\n",
            "Train Epoch: 1 [64000/193280 (33%)]\tLoss: 0.532881\n",
            "\n",
            "Train Epoch: 1 [96000/193280 (50%)]\tLoss: 0.485487\n",
            "\n",
            "Train Epoch: 1 [128000/193280 (66%)]\tLoss: 0.336164\n",
            "\n",
            "Train Epoch: 1 [160000/193280 (83%)]\tLoss: 0.128458\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:12<01:51, 12.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 1 [192000/193280 (99%)]\tLoss: 0.106937\n",
            "\n",
            "Train Epoch: 2 [0/193280 (0%)]\tLoss: 0.135489\n",
            "\n",
            "Train Epoch: 2 [32000/193280 (17%)]\tLoss: 0.121555\n",
            "\n",
            "Train Epoch: 2 [64000/193280 (33%)]\tLoss: 0.052690\n",
            "\n",
            "Train Epoch: 2 [96000/193280 (50%)]\tLoss: 0.014651\n",
            "\n",
            "Train Epoch: 2 [128000/193280 (66%)]\tLoss: 0.002533\n",
            "\n",
            "Train Epoch: 2 [160000/193280 (83%)]\tLoss: 0.007982\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:23<01:34, 11.86s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 2 [192000/193280 (99%)]\tLoss: 0.004510\n",
            "\n",
            "Train Epoch: 3 [0/193280 (0%)]\tLoss: 0.002381\n",
            "\n",
            "Train Epoch: 3 [32000/193280 (17%)]\tLoss: 0.004392\n",
            "\n",
            "Train Epoch: 3 [64000/193280 (33%)]\tLoss: 0.001433\n",
            "\n",
            "Train Epoch: 3 [96000/193280 (50%)]\tLoss: 0.001092\n",
            "\n",
            "Train Epoch: 3 [128000/193280 (66%)]\tLoss: 0.000709\n",
            "\n",
            "Train Epoch: 3 [160000/193280 (83%)]\tLoss: 0.008199\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [00:35<01:21, 11.66s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 3 [192000/193280 (99%)]\tLoss: 0.002274\n",
            "\n",
            "Train Epoch: 4 [0/193280 (0%)]\tLoss: 0.013704\n",
            "\n",
            "Train Epoch: 4 [32000/193280 (17%)]\tLoss: 0.000907\n",
            "\n",
            "Train Epoch: 4 [64000/193280 (33%)]\tLoss: 0.004618\n",
            "\n",
            "Train Epoch: 4 [96000/193280 (50%)]\tLoss: 0.002955\n",
            "\n",
            "Train Epoch: 4 [128000/193280 (66%)]\tLoss: 0.000467\n",
            "\n",
            "Train Epoch: 4 [160000/193280 (83%)]\tLoss: 0.000829\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [00:47<01:12, 12.07s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 4 [192000/193280 (99%)]\tLoss: 0.004390\n",
            "\n",
            "Train Epoch: 5 [0/193280 (0%)]\tLoss: 0.001720\n",
            "\n",
            "Train Epoch: 5 [32000/193280 (17%)]\tLoss: 0.001462\n",
            "\n",
            "Train Epoch: 5 [64000/193280 (33%)]\tLoss: 0.004501\n",
            "\n",
            "Train Epoch: 5 [96000/193280 (50%)]\tLoss: 0.000952\n",
            "\n",
            "Train Epoch: 5 [128000/193280 (66%)]\tLoss: 0.003096\n",
            "\n",
            "Train Epoch: 5 [160000/193280 (83%)]\tLoss: 0.000589\n",
            "\n",
            "Train Epoch: 5 [192000/193280 (99%)]\tLoss: 0.001467\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [01:05<01:09, 13.89s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 6 [0/193280 (0%)]\tLoss: 0.006065\n",
            "\n",
            "Train Epoch: 6 [32000/193280 (17%)]\tLoss: 0.001582\n",
            "\n",
            "Train Epoch: 6 [64000/193280 (33%)]\tLoss: 0.000849\n",
            "\n",
            "Train Epoch: 6 [96000/193280 (50%)]\tLoss: 0.029228\n",
            "\n",
            "Train Epoch: 6 [128000/193280 (66%)]\tLoss: 0.002124\n",
            "\n",
            "Train Epoch: 6 [160000/193280 (83%)]\tLoss: 0.004707\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [01:18<00:54, 13.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 6 [192000/193280 (99%)]\tLoss: 0.001505\n",
            "\n",
            "Train Epoch: 7 [0/193280 (0%)]\tLoss: 0.001233\n",
            "\n",
            "Train Epoch: 7 [32000/193280 (17%)]\tLoss: 0.007592\n",
            "\n",
            "Train Epoch: 7 [64000/193280 (33%)]\tLoss: 0.002469\n",
            "\n",
            "Train Epoch: 7 [96000/193280 (50%)]\tLoss: 0.000647\n",
            "\n",
            "Train Epoch: 7 [128000/193280 (66%)]\tLoss: 0.005678\n",
            "\n",
            "Train Epoch: 7 [160000/193280 (83%)]\tLoss: 0.002477\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [01:29<00:38, 12.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Epoch: 7 [192000/193280 (99%)]\tLoss: 0.003203\n",
            "\n",
            "Train Epoch: 8 [0/193280 (0%)]\tLoss: 0.001178\n",
            "\n",
            "Train Epoch: 8 [32000/193280 (17%)]\tLoss: 0.000492\n",
            "\n",
            "Train Epoch: 8 [64000/193280 (33%)]\tLoss: 0.000387\n",
            "\n",
            "Train Epoch: 8 [96000/193280 (50%)]\tLoss: 0.001792\n",
            "\n",
            "Train Epoch: 8 [128000/193280 (66%)]\tLoss: 0.001440\n",
            "\n",
            "Train Epoch: 8 [160000/193280 (83%)]\tLoss: 0.001111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:41<00:24, 12.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Epoch: 8 [192000/193280 (99%)]\tLoss: 0.000519\n",
            "\n",
            "Train Epoch: 9 [0/193280 (0%)]\tLoss: 0.000533\n",
            "\n",
            "Train Epoch: 9 [32000/193280 (17%)]\tLoss: 0.008184\n",
            "\n",
            "Train Epoch: 9 [64000/193280 (33%)]\tLoss: 0.000415\n",
            "\n",
            "Train Epoch: 9 [96000/193280 (50%)]\tLoss: 0.001067\n",
            "\n",
            "Train Epoch: 9 [128000/193280 (66%)]\tLoss: 0.000271\n",
            "\n",
            "Train Epoch: 9 [160000/193280 (83%)]\tLoss: 0.001528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:52<00:12, 12.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Epoch: 9 [192000/193280 (99%)]\tLoss: 0.000627\n",
            "\n",
            "Train Epoch: 10 [0/193280 (0%)]\tLoss: 0.001616\n",
            "\n",
            "Train Epoch: 10 [32000/193280 (17%)]\tLoss: 0.000621\n",
            "\n",
            "Train Epoch: 10 [64000/193280 (33%)]\tLoss: 0.012707\n",
            "\n",
            "Train Epoch: 10 [96000/193280 (50%)]\tLoss: 0.000581\n",
            "\n",
            "Train Epoch: 10 [128000/193280 (66%)]\tLoss: 0.004178\n",
            "\n",
            "Train Epoch: 10 [160000/193280 (83%)]\tLoss: 0.002036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:04<00:00, 12.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Epoch: 10 [192000/193280 (99%)]\tLoss: 0.001008\n",
            "model saved to nn.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, y_val = preprocess(val_df)\n",
        "_, val_dl = create_dls(x_val, y_val)\n",
        "criterion = nn.MSELoss()\n",
        "val_loss, val_r2= test(model, model.device, val_dl, criterion)\n",
        "print(f'Neural Network Validation Mean Squared Error: {val_loss:.6f}')\n",
        "print(f'Neural Network Validation R2: {val_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZcQqEQV7BA",
        "outputId": "11ce2fbd-6a3e-4363-b31b-41f8672b6d6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Validation Mean Squared Error: 0.001176\n",
            "Neural Network Validation R2: 0.9043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('nn.pth', weights_only=True))\n",
        "# print(\"Model's state_dict:\")\n",
        "# model.state_dict()"
      ],
      "metadata": {
        "id": "mOYlwVqjXoMV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA, Lasso & Linear Regression - Katelyn"
      ],
      "metadata": {
        "id": "KV9NQxxkBrNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "X_train = pca.fit_transform(x_tr)\n",
        "X_test = pca.transform(x_val)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "id": "Qs-FDMoSBvXf",
        "outputId": "1afbdd5c-8598-4f0c-b3a6-d16de0cebd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10205183 0.10191195]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#From Sklearn --> Lasso documentation\n",
        "\n",
        "#Setting alpha\n",
        "lasso = Lasso(alpha=0.00001)\n",
        "\n",
        "#Fitting to training data\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "#making y predictions based on the X_test, given by the PCA\n",
        "y_pred = lasso.predict(X_test)\n",
        "\n",
        "#Getting the MSE\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "#very low lasso coefficents"
      ],
      "metadata": {
        "id": "_nXR5cexENNO",
        "outputId": "2d9ad061-410c-44cd-9658-550904104a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.012236981126572913\n",
            "Coefficients: [ 0.01265718 -0.00163292]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize LDA and fit the model\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = linreg.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "#This does extremely poorly? It's likely that the lower number of features means the model does not perform as well.\n",
        "print('val r2:', r2_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "Ld6SL5soFShI",
        "outputId": "e973869f-8f21-40b9-e63b-1697a7f19998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.012236947492410354\n",
            "val r2: 0.004309198228256705\n"
          ]
        }
      ]
    }
  ]
}