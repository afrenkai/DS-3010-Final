{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7PwZu06ZuAuQLEWw+MczF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEvTRtNnDY2Q",
        "outputId": "4f4d1358-6479-4ccb-82a5-922b535b706a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS-3010-Final'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 111 (delta 44), reused 78 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (111/111), 17.56 MiB | 15.39 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/afrenkai/DS-3010-Final.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DS-3010-Final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiUGv3JMDiIb",
        "outputId": "f771c223-4206-47b5-dd26-df54cd901cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS-3010-Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Exc780BDznq",
        "outputId": "8bf0e640-7188-404a-dc57-f59b91a87902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cv.py  LICENSE\t   main.py\t\t\t README.md\n",
            "Data   Main.ipynb  new_3010_proj_work_ben.ipynb  requirements.txt\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from lightgbm import LGBMRegressor\n",
        "from torcheval.metrics import R2Score"
      ],
      "metadata": {
        "id": "KjqLGK9uDuvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('Data/SGEMM_train.csv')\n",
        "val_df = pd.read_csv('Data/SGEMM_val.csv')\n",
        "\n",
        "#TODO: read test data (already in data dir), see what's going on in lightgbm, get r2 for the neural net"
      ],
      "metadata": {
        "id": "jSFtZgO5EIYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "yFsL0jLPEah0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x, xmin, xmax, a, b):\n",
        "  '''\n",
        "  Restricts x values to range of [xmin, xmax]\n",
        "  '''\n",
        "  numerator = x - xmin\n",
        "  denominator = xmax - xmin\n",
        "  return (numerator / denominator) * (b - a) + a"
      ],
      "metadata": {
        "id": "VeZMpJ4fWVnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_combine = ['Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)']"
      ],
      "metadata": {
        "id": "Zg8_iVh1EoPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df: pd.DataFrame):\n",
        "  df['DELTA_RUNTIME'] = df.apply(\n",
        "      lambda row: np.mean([row['Run1 (ms)'], row['Run2 (ms)'], row['Run3 (ms)'], row['Run4 (ms)']]),\n",
        "      axis=1\n",
        "  )\n",
        "  for col in df.columns:\n",
        "    if col in cols_to_combine:\n",
        "      df = df.drop(col, axis = 1)\n",
        "  min = 0\n",
        "  max = 1\n",
        "\n",
        "\n",
        "  df = df.apply(\n",
        "      lambda row: (norm(row, row.min(), row.max(), min, max))\n",
        "  )\n",
        "  x = df.iloc[:, :14]\n",
        "  y = df.iloc[:, -1:]\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "t7Ldc6DaEWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "DsGLn9WEE1Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_tr, y_tr = preprocess(train_df)\n",
        "\n",
        "train_data = lgb.Dataset(x_tr, label=y_tr)\n",
        "x_val, y_val = preprocess(val_df)\n",
        "# Create a LightGBM dataset for testing with features X_val and labels Y_val,\n",
        "# and specify the reference dataset as train_data for consistent evaluation\n",
        "val_data = lgb.Dataset(x_val, label=y_val, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "}\n",
        "\n",
        "num_round = 100\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[\n",
        "                val_data])\n",
        "\n",
        "\n",
        "# Create an instance of the LightGBM Regressor with the MSE metric.\n",
        "model = LGBMRegressor(metric='mse')\n",
        "\n",
        "# Train the model using the training data.\n",
        "model.fit(x_tr, y_tr)\n",
        "\n",
        "y_train = model.predict(x_tr)\n",
        "y_v = model.predict(x_val)\n",
        "print(\"Training MSE:\", mse(y_tr, y_train))\n",
        "print(\"Validation MSE:\", mse(y_val, y_v))\n",
        "\n",
        "print('train r2:', r2_score(y_tr, y_train))\n",
        "print('val r2:', r2_score(y_val, y_v))"
      ],
      "metadata": {
        "id": "bWXyH5z-Lmnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fec63ad-df12-45b0-f03d-b3abd0266edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "Training MSE: 0.00012048252219381041\n",
            "Validation MSE: 0.0001191615359407208\n",
            "train r2: 0.9901529624802171\n",
            "val r2: 0.9903041142135522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Neural Net"
      ],
      "metadata": {
        "id": "GKjUYgsALmtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUNN(nn.Module):\n",
        "  def __init__(self, in_feat, out_feat):\n",
        "    super(GPUNN, self).__init__()\n",
        "    self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "    self.L1 = nn.Linear(in_feat, 64, device=self.device)\n",
        "    self.L2 = nn.Linear(64, out_feat, device = self.device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(64, device = self.device)\n",
        "    self.bn2 = nn.BatchNorm1d(out_feat, device = self.device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(self.relu(self.L1(x)))\n",
        "    x = self.bn2(self.L2(x))\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "TJtfhd5sLoz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, train_dl: DataLoader, batch_size, device, n_epochs, optimizer, criterion):\n",
        "  model.train()\n",
        "  for batch, (data, target) in enumerate(train_dl):\n",
        "\n",
        "    data, target = data.to(device).float(), target.to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                n_epochs, batch * len(data), len(train_dl.dataset),\n",
        "                100. * batch / len(train_dl), loss.item()))\n",
        "\n",
        "  torch.save(model.state_dict(), 'nn.pth')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jrBA-CHTOLEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    losses = []\n",
        "    r2s = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss = criterion(output, target)\n",
        "            # print('loss bien')\n",
        "            # print(f'Target Tensor: {target.detach().cpu().numpy()}\\n, Output Tensor:{output.detach().cpu().numpy()}\\n')\n",
        "            test_r2 = r2_score(target.detach().cpu().numpy(), output.detach().cpu().numpy())\n",
        "\n",
        "            # print('r2 bien')\n",
        "            losses.append(test_loss)\n",
        "            r2s.append(test_r2)\n",
        "\n",
        "\n",
        "    # print(test_loss / len(test_loader.dataset))\n",
        "    # print(np.mean(r2s))\n",
        "\n",
        "    return (np.mean([ten.detach().cpu().numpy() for ten in losses]), 0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-NN3jSkBVjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dls(x: pd.DataFrame, y:pd.DataFrame):\n",
        "  x = x.loc[:, :].values\n",
        "  y = y.loc[:, :].values\n",
        "  x_ten = torch.tensor(x)\n",
        "  y_ten = torch.tensor(y)\n",
        "  ds = TensorDataset(x_ten, y_ten)\n",
        "  dl = DataLoader(ds, batch_size = 32)\n",
        "  return ds, dl"
      ],
      "metadata": {
        "id": "8ZQiw6qdP-5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr, y_tr = preprocess(train_df)\n",
        "model = GPUNN(len(x_tr.columns), len(y_tr.columns))\n",
        "print(model)\n",
        "_, train_dl = create_dls(x_tr, y_tr)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
        "train(model, train_dl, 32, model.device, 10, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS8nqyTzNkF6",
        "outputId": "e8a50e23-9bc9-4156-8c08-bc88337d8332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUNN(\n",
            "  (L1): Linear(in_features=14, out_features=64, bias=True)\n",
            "  (L2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Train Epoch: 10 [0/193280 (0%)]\tLoss: 0.964049\n",
            "Train Epoch: 10 [3200/193280 (2%)]\tLoss: 0.965339\n",
            "Train Epoch: 10 [6400/193280 (3%)]\tLoss: 0.878680\n",
            "Train Epoch: 10 [9600/193280 (5%)]\tLoss: 0.909235\n",
            "Train Epoch: 10 [12800/193280 (7%)]\tLoss: 0.831005\n",
            "Train Epoch: 10 [16000/193280 (8%)]\tLoss: 0.744004\n",
            "Train Epoch: 10 [19200/193280 (10%)]\tLoss: 0.722842\n",
            "Train Epoch: 10 [22400/193280 (12%)]\tLoss: 0.766331\n",
            "Train Epoch: 10 [25600/193280 (13%)]\tLoss: 0.777860\n",
            "Train Epoch: 10 [28800/193280 (15%)]\tLoss: 0.710101\n",
            "Train Epoch: 10 [32000/193280 (17%)]\tLoss: 0.735097\n",
            "Train Epoch: 10 [35200/193280 (18%)]\tLoss: 0.659104\n",
            "Train Epoch: 10 [38400/193280 (20%)]\tLoss: 0.697198\n",
            "Train Epoch: 10 [41600/193280 (22%)]\tLoss: 0.650589\n",
            "Train Epoch: 10 [44800/193280 (23%)]\tLoss: 0.647873\n",
            "Train Epoch: 10 [48000/193280 (25%)]\tLoss: 0.586084\n",
            "Train Epoch: 10 [51200/193280 (26%)]\tLoss: 0.670288\n",
            "Train Epoch: 10 [54400/193280 (28%)]\tLoss: 0.617669\n",
            "Train Epoch: 10 [57600/193280 (30%)]\tLoss: 0.512884\n",
            "Train Epoch: 10 [60800/193280 (31%)]\tLoss: 0.588939\n",
            "Train Epoch: 10 [64000/193280 (33%)]\tLoss: 0.578412\n",
            "Train Epoch: 10 [67200/193280 (35%)]\tLoss: 0.452827\n",
            "Train Epoch: 10 [70400/193280 (36%)]\tLoss: 0.589898\n",
            "Train Epoch: 10 [73600/193280 (38%)]\tLoss: 0.445102\n",
            "Train Epoch: 10 [76800/193280 (40%)]\tLoss: 0.503563\n",
            "Train Epoch: 10 [80000/193280 (41%)]\tLoss: 0.461865\n",
            "Train Epoch: 10 [83200/193280 (43%)]\tLoss: 0.511136\n",
            "Train Epoch: 10 [86400/193280 (45%)]\tLoss: 0.410631\n",
            "Train Epoch: 10 [89600/193280 (46%)]\tLoss: 0.348256\n",
            "Train Epoch: 10 [92800/193280 (48%)]\tLoss: 0.342525\n",
            "Train Epoch: 10 [96000/193280 (50%)]\tLoss: 0.378229\n",
            "Train Epoch: 10 [99200/193280 (51%)]\tLoss: 0.435631\n",
            "Train Epoch: 10 [102400/193280 (53%)]\tLoss: 0.417035\n",
            "Train Epoch: 10 [105600/193280 (55%)]\tLoss: 0.381850\n",
            "Train Epoch: 10 [108800/193280 (56%)]\tLoss: 0.332203\n",
            "Train Epoch: 10 [112000/193280 (58%)]\tLoss: 0.409554\n",
            "Train Epoch: 10 [115200/193280 (60%)]\tLoss: 0.339277\n",
            "Train Epoch: 10 [118400/193280 (61%)]\tLoss: 0.327652\n",
            "Train Epoch: 10 [121600/193280 (63%)]\tLoss: 0.323134\n",
            "Train Epoch: 10 [124800/193280 (65%)]\tLoss: 0.381303\n",
            "Train Epoch: 10 [128000/193280 (66%)]\tLoss: 0.313847\n",
            "Train Epoch: 10 [131200/193280 (68%)]\tLoss: 0.301638\n",
            "Train Epoch: 10 [134400/193280 (70%)]\tLoss: 0.316049\n",
            "Train Epoch: 10 [137600/193280 (71%)]\tLoss: 0.213770\n",
            "Train Epoch: 10 [140800/193280 (73%)]\tLoss: 0.304952\n",
            "Train Epoch: 10 [144000/193280 (75%)]\tLoss: 0.304503\n",
            "Train Epoch: 10 [147200/193280 (76%)]\tLoss: 0.297681\n",
            "Train Epoch: 10 [150400/193280 (78%)]\tLoss: 0.188681\n",
            "Train Epoch: 10 [153600/193280 (79%)]\tLoss: 0.184801\n",
            "Train Epoch: 10 [156800/193280 (81%)]\tLoss: 0.175433\n",
            "Train Epoch: 10 [160000/193280 (83%)]\tLoss: 0.183797\n",
            "Train Epoch: 10 [163200/193280 (84%)]\tLoss: 0.242865\n",
            "Train Epoch: 10 [166400/193280 (86%)]\tLoss: 0.205818\n",
            "Train Epoch: 10 [169600/193280 (88%)]\tLoss: 0.192477\n",
            "Train Epoch: 10 [172800/193280 (89%)]\tLoss: 0.212501\n",
            "Train Epoch: 10 [176000/193280 (91%)]\tLoss: 0.210184\n",
            "Train Epoch: 10 [179200/193280 (93%)]\tLoss: 0.105595\n",
            "Train Epoch: 10 [182400/193280 (94%)]\tLoss: 0.161585\n",
            "Train Epoch: 10 [185600/193280 (96%)]\tLoss: 0.172579\n",
            "Train Epoch: 10 [188800/193280 (98%)]\tLoss: 0.173106\n",
            "Train Epoch: 10 [192000/193280 (99%)]\tLoss: 0.152290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, y_val = preprocess(val_df)\n",
        "_, val_dl = create_dls(x_val, y_val)\n",
        "criterion = nn.MSELoss()\n",
        "val_loss, _= test(model, model.device, val_dl, criterion)\n",
        "print(f'Neural Network Validation Mean Squared Error: {val_loss}')\n",
        "# print(f'Neural Network Validation R2: {val_r2}')\n",
        "# print(len(tar))\n",
        "# print(len(lab))\n",
        "# print(tar)\n",
        "# print(type(lab[0]))\n",
        "# print(type(tar[0]))\n",
        "# metric = R2Score()\n",
        "# metric.update(lab[0], tar[0])\n",
        "# metric.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZcQqEQV7BA",
        "outputId": "f46cbc18-1280-466c-f656-d32600493d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Validation Mean Squared Error: 0.1372702717781067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('nn.pth', weights_only=True))\n",
        "# # Print model's state_dict\n",
        "# print(\"Model's state_dict:\")\n",
        "# model.state_dict()"
      ],
      "metadata": {
        "id": "mOYlwVqjXoMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myDmHjmWb8Nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}