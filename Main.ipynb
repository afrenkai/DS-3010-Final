{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEvTRtNnDY2Q",
        "outputId": "f4dc06d1-01f2-4410-ad3d-9202028fbad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS-3010-Final'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 122 (delta 51), reused 80 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (122/122), 17.56 MiB | 12.83 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/afrenkai/DS-3010-Final.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DS-3010-Final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiUGv3JMDiIb",
        "outputId": "72eb6e14-565a-4112-974f-65e34454dc96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS-3010-Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Exc780BDznq",
        "outputId": "41b3b8b7-ec82-4d34-8a77-a460880fec45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cv.py  LICENSE\t   new_3010_proj_work_ben.ipynb  requirements.txt\n",
            "Data   Main.ipynb  README.md\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from lightgbm import LGBMRegressor\n",
        "from torcheval.metrics import R2Score"
      ],
      "metadata": {
        "id": "KjqLGK9uDuvW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('Data/SGEMM_train.csv')\n",
        "val_df = pd.read_csv('Data/SGEMM_val.csv')\n",
        "\n",
        "#TODO: read test data (already in data dir), see what's going on in lightgbm, get r2 for the neural net"
      ],
      "metadata": {
        "id": "jSFtZgO5EIYi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "yFsL0jLPEah0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x, xmin, xmax, a, b):\n",
        "  '''\n",
        "  Restricts x values to range of [xmin, xmax]\n",
        "  '''\n",
        "  numerator = x - xmin\n",
        "  denominator = xmax - xmin\n",
        "  return (numerator / denominator) * (b - a) + a"
      ],
      "metadata": {
        "id": "VeZMpJ4fWVnS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_combine = ['Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)']"
      ],
      "metadata": {
        "id": "Zg8_iVh1EoPf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df: pd.DataFrame):\n",
        "  df['DELTA_RUNTIME'] = df.apply(\n",
        "      lambda row: np.mean([row['Run1 (ms)'], row['Run2 (ms)'], row['Run3 (ms)'], row['Run4 (ms)']]),\n",
        "      axis=1\n",
        "  )\n",
        "  for col in df.columns:\n",
        "    if col in cols_to_combine:\n",
        "      df = df.drop(col, axis = 1)\n",
        "  min = 0\n",
        "  max = 1\n",
        "\n",
        "\n",
        "  df = df.apply(\n",
        "      lambda row: (norm(row, row.min(), row.max(), min, max))\n",
        "  )\n",
        "  x = df.iloc[:, :14]\n",
        "  y = df.iloc[:, -1:]\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "t7Ldc6DaEWb5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "DsGLn9WEE1Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_tr, y_tr = preprocess(train_df)\n",
        "\n",
        "train_data = lgb.Dataset(x_tr, label=y_tr)\n",
        "x_val, y_val = preprocess(val_df)\n",
        "# Create a LightGBM dataset for testing with features X_val and labels Y_val,\n",
        "# and specify the reference dataset as train_data for consistent evaluation\n",
        "val_data = lgb.Dataset(x_val, label=y_val, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "}\n",
        "\n",
        "num_round = 100\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[\n",
        "                val_data])\n",
        "\n",
        "\n",
        "# Create an instance of the LightGBM Regressor with the RMSE metric.\n",
        "model = LGBMRegressor(metric='mse')\n",
        "\n",
        "# Train the model using the training data.\n",
        "model.fit(x_tr, y_tr)\n",
        "\n",
        "y_train = model.predict(x_tr)\n",
        "y_v = model.predict(x_val)\n",
        "print(\"Training MSE:\", mse(y_tr, y_train))\n",
        "print(\"Validation MSE:\", mse(y_val, y_v))\n",
        "\n",
        "print('train r2:', r2_score(y_tr, y_train))\n",
        "print('val r2:', r2_score(y_val, y_v))"
      ],
      "metadata": {
        "id": "bWXyH5z-Lmnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797b88ff-4b95-4656-f5b5-e714cc056e82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 40\n",
            "[LightGBM] [Info] Number of data points in the train set: 193280, number of used features: 14\n",
            "[LightGBM] [Info] Start training from score 0.061354\n",
            "Training MSE: 0.00012048252219381041\n",
            "Validation MSE: 0.0001191615359407208\n",
            "train r2: 0.9901529624802171\n",
            "val r2: 0.9903041142135522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Neural Net"
      ],
      "metadata": {
        "id": "GKjUYgsALmtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUNN(nn.Module):\n",
        "  def __init__(self, in_feat, out_feat):\n",
        "    super(GPUNN, self).__init__()\n",
        "    self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "    self.L1 = nn.Linear(in_feat, 64, device=self.device)\n",
        "    self.L2 = nn.Linear(64, out_feat, device = self.device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(64, device = self.device)\n",
        "    self.bn2 = nn.BatchNorm1d(out_feat, device = self.device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(self.relu(self.L1(x)))\n",
        "    x = self.bn2(self.L2(x))\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "TJtfhd5sLoz2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, train_dl: DataLoader, batch_size, device, n_epochs, optimizer, criterion):\n",
        "  model.train()\n",
        "  for batch, (data, target) in enumerate(train_dl):\n",
        "\n",
        "    data, target = data.to(device).float(), target.to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                n_epochs, batch * len(data), len(train_dl.dataset),\n",
        "                100. * batch / len(train_dl), loss.item()))\n",
        "\n",
        "  torch.save(model.state_dict(), 'nn.pth')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jrBA-CHTOLEl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    losses = []\n",
        "    r2s = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device).float(), target.to(device).float()\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss = criterion(output, target)\n",
        "            # print('loss bien')\n",
        "            # print(f'Target Tensor: {target.detach().cpu().numpy()}\\n, Output Tensor:{output.detach().cpu().numpy()}\\n')\n",
        "            test_r2 = r2_score(target.detach().cpu().numpy(), output.detach().cpu().numpy())\n",
        "\n",
        "            # print('r2 bien')\n",
        "            losses.append(test_loss)\n",
        "            r2s.append(test_r2)\n",
        "\n",
        "\n",
        "    # print(test_loss / len(test_loader.dataset))\n",
        "    # print(np.mean(r2s))\n",
        "\n",
        "    return (np.mean([ten.detach().cpu().numpy() for ten in losses]), 0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-NN3jSkBVjKt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dls(x: pd.DataFrame, y:pd.DataFrame):\n",
        "  x = x.loc[:, :].values\n",
        "  y = y.loc[:, :].values\n",
        "  x_ten = torch.tensor(x)\n",
        "  y_ten = torch.tensor(y)\n",
        "  ds = TensorDataset(x_ten, y_ten)\n",
        "  dl = DataLoader(ds, batch_size = 32)\n",
        "  return ds, dl"
      ],
      "metadata": {
        "id": "8ZQiw6qdP-5v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr, y_tr = preprocess(train_df)\n",
        "model = GPUNN(len(x_tr.columns), len(y_tr.columns))\n",
        "print(model)\n",
        "_, train_dl = create_dls(x_tr, y_tr)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
        "train(model, train_dl, 32, model.device, 10, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS8nqyTzNkF6",
        "outputId": "cdf5bc05-59ef-4018-f6e0-5bb9da948267"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUNN(\n",
            "  (L1): Linear(in_features=14, out_features=64, bias=True)\n",
            "  (L2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Train Epoch: 10 [0/193280 (0%)]\tLoss: 1.016734\n",
            "Train Epoch: 10 [3200/193280 (2%)]\tLoss: 0.969022\n",
            "Train Epoch: 10 [6400/193280 (3%)]\tLoss: 0.932685\n",
            "Train Epoch: 10 [9600/193280 (5%)]\tLoss: 0.931066\n",
            "Train Epoch: 10 [12800/193280 (7%)]\tLoss: 0.841159\n",
            "Train Epoch: 10 [16000/193280 (8%)]\tLoss: 0.775493\n",
            "Train Epoch: 10 [19200/193280 (10%)]\tLoss: 0.733320\n",
            "Train Epoch: 10 [22400/193280 (12%)]\tLoss: 0.786085\n",
            "Train Epoch: 10 [25600/193280 (13%)]\tLoss: 0.774920\n",
            "Train Epoch: 10 [28800/193280 (15%)]\tLoss: 0.714361\n",
            "Train Epoch: 10 [32000/193280 (17%)]\tLoss: 0.741342\n",
            "Train Epoch: 10 [35200/193280 (18%)]\tLoss: 0.669209\n",
            "Train Epoch: 10 [38400/193280 (20%)]\tLoss: 0.711002\n",
            "Train Epoch: 10 [41600/193280 (22%)]\tLoss: 0.658281\n",
            "Train Epoch: 10 [44800/193280 (23%)]\tLoss: 0.648569\n",
            "Train Epoch: 10 [48000/193280 (25%)]\tLoss: 0.589879\n",
            "Train Epoch: 10 [51200/193280 (26%)]\tLoss: 0.673540\n",
            "Train Epoch: 10 [54400/193280 (28%)]\tLoss: 0.624558\n",
            "Train Epoch: 10 [57600/193280 (30%)]\tLoss: 0.508360\n",
            "Train Epoch: 10 [60800/193280 (31%)]\tLoss: 0.592554\n",
            "Train Epoch: 10 [64000/193280 (33%)]\tLoss: 0.583590\n",
            "Train Epoch: 10 [67200/193280 (35%)]\tLoss: 0.444446\n",
            "Train Epoch: 10 [70400/193280 (36%)]\tLoss: 0.591826\n",
            "Train Epoch: 10 [73600/193280 (38%)]\tLoss: 0.448180\n",
            "Train Epoch: 10 [76800/193280 (40%)]\tLoss: 0.510580\n",
            "Train Epoch: 10 [80000/193280 (41%)]\tLoss: 0.457385\n",
            "Train Epoch: 10 [83200/193280 (43%)]\tLoss: 0.510793\n",
            "Train Epoch: 10 [86400/193280 (45%)]\tLoss: 0.418819\n",
            "Train Epoch: 10 [89600/193280 (46%)]\tLoss: 0.365524\n",
            "Train Epoch: 10 [92800/193280 (48%)]\tLoss: 0.353881\n",
            "Train Epoch: 10 [96000/193280 (50%)]\tLoss: 0.392742\n",
            "Train Epoch: 10 [99200/193280 (51%)]\tLoss: 0.430058\n",
            "Train Epoch: 10 [102400/193280 (53%)]\tLoss: 0.415435\n",
            "Train Epoch: 10 [105600/193280 (55%)]\tLoss: 0.375522\n",
            "Train Epoch: 10 [108800/193280 (56%)]\tLoss: 0.357234\n",
            "Train Epoch: 10 [112000/193280 (58%)]\tLoss: 0.403525\n",
            "Train Epoch: 10 [115200/193280 (60%)]\tLoss: 0.328572\n",
            "Train Epoch: 10 [118400/193280 (61%)]\tLoss: 0.331395\n",
            "Train Epoch: 10 [121600/193280 (63%)]\tLoss: 0.330064\n",
            "Train Epoch: 10 [124800/193280 (65%)]\tLoss: 0.381072\n",
            "Train Epoch: 10 [128000/193280 (66%)]\tLoss: 0.308792\n",
            "Train Epoch: 10 [131200/193280 (68%)]\tLoss: 0.305042\n",
            "Train Epoch: 10 [134400/193280 (70%)]\tLoss: 0.313655\n",
            "Train Epoch: 10 [137600/193280 (71%)]\tLoss: 0.219351\n",
            "Train Epoch: 10 [140800/193280 (73%)]\tLoss: 0.310190\n",
            "Train Epoch: 10 [144000/193280 (75%)]\tLoss: 0.302534\n",
            "Train Epoch: 10 [147200/193280 (76%)]\tLoss: 0.294676\n",
            "Train Epoch: 10 [150400/193280 (78%)]\tLoss: 0.189738\n",
            "Train Epoch: 10 [153600/193280 (79%)]\tLoss: 0.192345\n",
            "Train Epoch: 10 [156800/193280 (81%)]\tLoss: 0.174669\n",
            "Train Epoch: 10 [160000/193280 (83%)]\tLoss: 0.183990\n",
            "Train Epoch: 10 [163200/193280 (84%)]\tLoss: 0.258903\n",
            "Train Epoch: 10 [166400/193280 (86%)]\tLoss: 0.205060\n",
            "Train Epoch: 10 [169600/193280 (88%)]\tLoss: 0.191158\n",
            "Train Epoch: 10 [172800/193280 (89%)]\tLoss: 0.213400\n",
            "Train Epoch: 10 [176000/193280 (91%)]\tLoss: 0.210318\n",
            "Train Epoch: 10 [179200/193280 (93%)]\tLoss: 0.110974\n",
            "Train Epoch: 10 [182400/193280 (94%)]\tLoss: 0.162365\n",
            "Train Epoch: 10 [185600/193280 (96%)]\tLoss: 0.175540\n",
            "Train Epoch: 10 [188800/193280 (98%)]\tLoss: 0.170951\n",
            "Train Epoch: 10 [192000/193280 (99%)]\tLoss: 0.156330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, y_val = preprocess(val_df)\n",
        "_, val_dl = create_dls(x_val, y_val)\n",
        "criterion = nn.MSELoss()\n",
        "val_loss, _= test(model, model.device, val_dl, criterion)\n",
        "print(f'Neural Network Validation Mean Squared Error: {val_loss}')\n",
        "# print(f'Neural Network Validation R2: {val_r2}')\n",
        "# print(len(tar))\n",
        "# print(len(lab))\n",
        "# print(tar)\n",
        "# print(type(lab[0]))\n",
        "# print(type(tar[0]))\n",
        "# metric = R2Score()\n",
        "# metric.update(lab[0], tar[0])\n",
        "# metric.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqZcQqEQV7BA",
        "outputId": "01949881-2a0e-4379-93e2-186bf7627a1e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Validation Mean Squared Error: 0.1422225832939148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('nn.pth', weights_only=True))\n",
        "# # Print model's state_dict\n",
        "# print(\"Model's state_dict:\")\n",
        "# model.state_dict()"
      ],
      "metadata": {
        "id": "mOYlwVqjXoMV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA, Lasso & Linear Regression - Katelyn"
      ],
      "metadata": {
        "id": "KV9NQxxkBrNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "X_train = pca.fit_transform(x_tr)\n",
        "X_test = pca.transform(x_val)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "id": "Qs-FDMoSBvXf",
        "outputId": "3b2d31fe-0fbf-4b53-b2dc-eb28a677c73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10205183 0.10191195]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#From Sklearn --> Lasso documentation\n",
        "\n",
        "#Setting alpha\n",
        "lasso = Lasso(alpha=0.00001)\n",
        "\n",
        "#Fitting to training data\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "#making y predictions based on the X_test, given by the PCA\n",
        "y_pred = lasso.predict(X_test)\n",
        "\n",
        "#Getting the MSE\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "#very low lasso coefficents"
      ],
      "metadata": {
        "id": "_nXR5cexENNO",
        "outputId": "25fbaa8f-762e-44b7-ab69-f51b5f2a4a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.012236981126572913\n",
            "Coefficients: [ 0.01265718 -0.00163292]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize LDA and fit the model\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = linreg.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "#This does very well, but the other methods are preferable for exploring.\n",
        "print('train r2:', r2_score(y_tr, y_train))"
      ],
      "metadata": {
        "id": "Ld6SL5soFShI",
        "outputId": "564b7d04-9081-4ff8-fdc9-7a350525396b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.012236947492410354\n",
            "train r2: 0.9901529624802171\n"
          ]
        }
      ]
    }
  ]
}